{ "file": "arch/x86/kvm/mmu/mmu.c", "scenario_id": "arch/x86/kvm/mmu/mmu.c-0001", "title": "Live Lock in shadow_mmu_try_split_huge_pages due to persistent rescheduling", "concept": "Concurrency/Resource Abuse: A task can get stuck in an infinite loop while trying to split huge pages if the system is busy, dropping the lock and restarting without making progress.", "attacker_model": "local_unprivileged | remote", "preconditions": ["Eager page splitting enabled (dirty logging)", "Heavy CPU load on the host (persistent need_resched())"], "description": "Vulnerability Path: Setup: Enable dirty logging on a memslot populated with huge pages. Trigger: `kvm_mmu_slot_try_split_huge_pages` calls `shadow_mmu_try_split_huge_pages`. Mechanism: The function iterates over rmaps. Inside the loop, it calls `shadow_mmu_try_split_huge_page`. This function checks `need_topup_split_caches_or_resched`. If `need_resched()` is true (due to high load), it unlocks `mmu_lock`, calls `cond_resched`, tops up caches, re-locks, and returns `-EAGAIN`. The loop in `shadow_mmu_try_split_huge_pages` catches `-EAGAIN` and executes `goto restart`, which resets the iterator to the beginning. If the system remains busy, `need_resched()` stays true, and the loop repeats indefinitely, constantly retrying the same first huge page but yielding before splitting it. Impact: The `ioctl` enabling dirty logging (or other operations invoking split) hangs indefinitely or takes an excessive amount of time, denying service to the control plane.", "classification": "DoS", "impact": "persistent DoS", "likelihood": "medium", "verdict": "confirmed_vuln", "context": "kernel_core", "evidence": ["arch/x86/kvm/mmu/mmu.c:6635: if (need_topup_split_caches_or_resched(kvm))", "arch/x86/kvm/mmu/mmu.c:6637: cond_resched()", "arch/x86/kvm/mmu/mmu.c:6665: if (!r || r == -EAGAIN) goto restart;"], "derived_from": [], "proposed_fix_summary": "Use `cond_resched_rwlock_write` or similar mechanism that doesn't require restarting the iterator from the beginning, or ensure progress is made before yielding." }
{ "file": "arch/x86/kvm/mmu/mmu.c", "scenario_id": "arch/x86/kvm/mmu/mmu.c-0002", "title": "Performance DoS via RMAP Recycle Threshold Thrashing", "concept": "Resource Abuse: Malicious guest can force expensive RMAP zapping and TLB flushing by creating just enough mappings to exceed the RMAP recycle threshold.", "attacker_model": "container_guest", "preconditions": ["Shadow Paging enabled (nested virtualization or old hardware)", "Guest ability to map same GFN multiple times"], "description": "Vulnerability Path: Setup: Guest creates many page table entries pointing to the same GFN (aliasing). Trigger: Guest adds the 1001st mapping. Mechanism: `rmap_add` checks `rmap_count > RMAP_RECYCLE_THRESHOLD` (1000). If true, it calls `kvm_zap_all_rmap_sptes` which destroys all existing shadow mappings for that GFN and flushes remote TLBs. This is an O(N) operation + IPIs. The guest can immediately re-fault the pages in. By oscillating around the threshold (add, zap, fault, add...), the guest can force the host to spend excessive CPU cycles managing shadow pages and flushing TLBs. Impact: Significant host CPU consumption, degrading performance for other tenants.", "classification": "resource-exhaustion", "impact": "persistent DoS", "likelihood": "medium", "verdict": "hardening_only", "context": "driver", "evidence": ["arch/x86/kvm/mmu/mmu.c:1653: #define RMAP_RECYCLE_THRESHOLD 1000", "arch/x86/kvm/mmu/mmu.c:1669: kvm_zap_all_rmap_sptes(kvm, rmap_head);"], "derived_from": [], "proposed_fix_summary": "Increase the threshold or implement a more graceful degradation (e.g. disable rmap for that GFN and use brute force flush) instead of immediate zapping." }
{ "file": "arch/x86/kvm/mmu/mmu.c", "scenario_id": "arch/x86/kvm/mmu/mmu.c-0003", "title": "Potential Recursion Limit in mmu_zap_unsync_children", "concept": "Resource Abuse: Deeply nested unsync shadow pages could potentially cause stack overflow during zapping.", "attacker_model": "container_guest", "preconditions": ["Shadow Paging", "Deep page tables (5-level or nested)"], "description": "Vulnerability Path: Setup: Guest creates a complex hierarchy of unsync shadow pages. Trigger: Zapping a parent page. Mechanism: `mmu_zap_unsync_children` calls `mmu_unsync_walk`, which calls `__mmu_unsync_walk`. `__mmu_unsync_walk` is recursive. It iterates over children and recurses. If the shadow page table structure is deep (e.g. through nested virtualization or specific configurations allowing deep chains of unsync pages), the recursion might consume significant stack space. While x86 page tables are limited in depth (4 or 5), if KVM's unsync logic allows chains that don't strictly follow the tree height (unlikely but possible if `parent_ptes` logic allows cycles or DAGs), stack overflow is possible. Even with standard depth, interrupt overhead + recursion might be tight. Impact: Kernel stack overflow / crash.", "classification": "resource-exhaustion", "impact": "kernel crash", "likelihood": "low", "verdict": "hardening_only", "context": "kernel_core", "evidence": ["arch/x86/kvm/mmu/mmu.c:2660: ret = __mmu_unsync_walk(child, pvec);"], "derived_from": [], "proposed_fix_summary": "Convert recursive `__mmu_unsync_walk` to iterative." }
