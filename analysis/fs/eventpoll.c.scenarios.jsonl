{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0001","title":"Nested epoll depth check bypass via concurrent tree mutation","concept":"Loop detection uses cached generation under RCU and can miss deep chains when concurrent epoll_ctl adds backpaths","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","user namespaces enabled"],"description":"Setup two epoll instances in different namespaces and heavily thread epoll_ctl(ADD/MOD) while another thread closes/duplicates fds. The loop_check_gen caching in ep_loop_check_proc() runs under RCU without locking; simultaneous insertions can advance loop depth after the generation snapshot, letting reverse_path_check() skip a newly formed backpath before ep_insert() completes. This creates a deeper-than-allowed nesting where callbacks can recurse and re-enter ep_poll_callback with stale assumptions, enabling starvation or UAF via overflowed recursion.","classification":"race-condition","impact":"persistent DoS","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:2077-2150: loop depth and reverse_path_check use generation counters without strong exclusion","fs/eventpoll.c:1564-1684: ep_insert performs reverse_path_check after attaching epi while other threads can mutate refs list"],"derived_from":[],"proposed_fix_summary":"Harden loop detection with atomic generation increments and locking around refs traversal, aborting on concurrent mutations."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0002","title":"Attach_epitem allocation retry races with concurrent close","concept":"attach_epitem drops f_lock to allocate epitems_head and retries, allowing file->f_ep to be cleared concurrently","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Create many epoll_ctl ADD operations on a rapidly closing fd while memory pressure forces kmem_cache_zalloc retries in attach_epitem(). The function drops f_lock when head is NULL and re-enters after allocation. If another thread closes the file and clears f_ep between retries, the first caller may hlist_add the epi into a stale head that will be freed in unlist_file(), leaving dangling fllink used by callbacks.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1527-1558: attach_epitem allocates head with retry dropping f_lock","fs/eventpoll.c:300-313: unlist_file frees ephead when list appears empty"],"derived_from":[],"proposed_fix_summary":"Avoid dropping f_lock in attach_epitem by preallocating head or retrying with head held; validate f_ep after allocation before list insertion."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0003","title":"EPOLLWAKEUP source lifetime race on delete paths","concept":"Wakeup_source attached during ep_insert can be freed while callbacks still execute because destroy happens without draining concurrent poll callbacks","attacker_model":"local_unprivileged","preconditions":["CONFIG_PM_WAKELOCKS","EPOLLWAKEUP used"],"description":"User toggles EPOLLWAKEUP via EPOLL_CTL_MOD while stressing ep_poll callbacks from IRQ-driven devices. ep_destroy_wakeup_source() unregisters the wakeup_source after synchronize_rcu, but poll callbacks may still dereference epi->ws because ep_unregister_pollwait does not wait for in-flight wakeups before list_del. Rapid MOD/DEL cycles can lead to wakeup_source_unregister on a pointer still used by ep_pm_stay_awake in callback, causing UAF or sleep state inconsistencies.","classification":"UAF","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1511-1525: ep_destroy_wakeup_source replaces ws and unregisters after synchronize_rcu","fs/eventpoll.c:1666-1676: ep_pm_stay_awake uses epi wakeup_source inside ready path without additional ref"],"derived_from":[],"proposed_fix_summary":"Refcount wakeup_source use in ep_pm_stay_awake or delay unregister until callbacks quiesce via completion per epitem."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0004","title":"Per-user watch counter underflow on concurrent ep_remove_safe during cleanup","concept":"percpu_counter_dec is executed multiple times when ep_remove_safe races with eventpoll_release_file teardown","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","ability to rapidly fork/close"],"description":"Drive epoll_ctl ADDs then trigger close of target fds while another thread calls epoll_close on the epoll file. ep_clear_and_put walks the rbr tree and calls ep_remove_safe while eventpoll_release_file may also remove the same epi with force=true. Both paths call percpu_counter_dec(&user->epoll_watches) without coordination, causing underflow and wrapping the per-user quota, letting attacker exceed max_user_watches and retain epitems after quota exhaustion.","classification":"refcount","impact":"boundary bypass","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:886-887: percpu_counter_dec done during removal","fs/eventpoll.c:899-935: ep_clear_and_put iterates and removes epitems concurrently with release paths"],"derived_from":[],"proposed_fix_summary":"Guard per-user counter with atomic flag on epitem to ensure decrement happens once, or centralize removal accounting."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0005","title":"Ready list exposure after move to ovflist under lock gaps","concept":"ep_start_scan/ep_done_scan move entries between rdllist and ovflist without full serialization, allowing stale pointer traversal by callbacks","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"An attacker triggers nested wakeups while ep_poll is transferring events. ep_events_available checks rdllist and ovflist without lock; ep_try_send_events then grabs ep->lock and moves entries to ovflist. Meanwhile ep_poll_callback may concurrently traverse rdllist and manipulate rdllink. Because ovflist uses single-linked pointers without RCU, a callback can follow epi->next pointing to EP_UNACTIVE_PTR and later reused epi, causing stale pointer use and ready queue corruption.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:390-394: ep_events_available reads rdllist/ovflist without lock","fs/eventpoll.c:1178-1285: rdllist/ovflist manipulation under ep->lock with single-linked next"],"derived_from":[],"proposed_fix_summary":"Protect ovflist transitions with RCU or convert to list_head; ensure callbacks check EP_UNACTIVE_PTR under lock before dereference."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0006","title":"Busy-poll NAPI ID reuse allowing cross-epoll wake steering","concept":"napi_id stored per epitem without clearing on reuse lets attacker reclaim freed struct epitem and leak wakeups","attacker_model":"local_unprivileged","preconditions":["CONFIG_NET_RX_BUSY_POLL","able to attach sockets with busy_poll"],"description":"Attach a busy-poll capable socket, trigger ep_remove_safe via close, and rapidly re-add a different socket before kfree_rcu of epi. ep_set_busy_poll_napi_id stores napi_id once during insert and it is not cleared before RCU free. A wakeup delivered to old whead referencing freed epi may still use cached napi_id in ep_busy_loop, steering busy polling to an attacker-controlled socket and leaking cross-queue packets or causing CPU hog.","classification":"race-condition","impact":"info leak","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1663-1669: ep_set_busy_poll_napi_id called at insert without cleanup before kfree_rcu","fs/eventpoll.c:396-400: busy_loop_ep_timeout consults ep busy poll fields shared across epitems"],"derived_from":[],"proposed_fix_summary":"Zero busy-poll metadata before kfree_rcu or fence busy_loop against epitem reuse via generation counters."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0007","title":"Epoll file duplication bypasses loop detection with stale gen","concept":"dup2 and pidfd_getfd allow passing epoll files without updating gen tracking, letting nested insertion slip past ep_loop_check","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","pidfd support"],"description":"Create epoll A and B, dup B via pidfd_getfd into another process, and concurrently insert A into B while B into A across namespaces. ep_loop_check_proc relies on per-ep gen cache and sets inserting_into globally. The duplicated file does not update gen or refs while outstanding references move to new task, letting loop_check depth miss the new path until after ep_insert completes, resulting in recursive wake loops and stack exhaustion.","classification":"logic-bypass","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:2077-2150: loop detection uses global inserting_into and cached gen without pinning tasks","fs/eventpoll.c:1564-1684: ep_insert performs loop_check before poll callbacks start"],"derived_from":[],"proposed_fix_summary":"Pin ep file during loop checks and invalidate gen on fd duplication/installation; re-evaluate depth after install."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0008","title":"Userfaultfd interaction with epoll_ctl cleanup races","concept":"ep_remove_wait_queue may miss wakeup removal when userfaultfd maps copy pages, leaving dangling wait entry to freed epitem","attacker_model":"local_unprivileged","preconditions":["CONFIG_USERFAULTFD","EPOLL_CTL_DEL on userfaultfd"],"description":"Register a userfaultfd with EPOLLIN/OUT, trigger faults that block in userfaultfd_ctx, then concurrently call epoll_ctl DEL while closing the userfaultfd. ep_remove_wait_queue relies on waitqueue active state and whead pointer; if userfaultfd_wake() runs while ep_unregister_pollwait tears down, the wait entry can remain queued pointing to freed eppoll_entry causing later wake to dereference freed memory.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:663-691: ep_remove_wait_queue manipulates waitqueue without refcounting epi","fs/eventpoll.c:845-885: __ep_remove frees epi via kfree_rcu after unregistering pollwait"],"derived_from":[],"proposed_fix_summary":"Use refcounted wait entries or synchronize with waitqueue removal via completion before freeing epitem."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0009","title":"Epoll ioctl wakeups with stale task state causing missed interruptibility","concept":"ep_poll uses TASK_INTERRUPTIBLE without full memory barrier when adding to waitqueue leading to lost wakeups under CPU hotplug","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","SMP"],"description":"During ep_poll, the thread sets TASK_INTERRUPTIBLE and conditionally adds to wq. On hotplug or scheduling migration, a concurrent wakeup may happen before __add_wait_queue_exclusive runs, and because waitqueue_active is barrierless, wakeup may be lost, leaving thread sleeping while events available, enabling starvation and potential DoS by forcing threads to never wake until timeout.","classification":"race-condition","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1969-2023: ep_poll sets TASK_INTERRUPTIBLE then adds wait queue using barrierless waitqueue_active check"],"derived_from":[],"proposed_fix_summary":"Add smp_mb__after_atomic or use prepare_to_wait_exclusive to pair state change with queue insertion."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0010","title":"Busy poll preference toggling without serialization","concept":"prefer_busy_poll flag toggled via sysfs and epoll busy_poll loop shares ep->lock with other ready list operations causing starvation","attacker_model":"local_unprivileged","preconditions":["CONFIG_NET_RX_BUSY_POLL","sysctl busy_poll"],"description":"Attacker writes busy_poll sysctls to flip prefer_busy_poll while issuing epoll_wait on sockets. ep_busy_loop reads ep->prefer_busy_poll without locking and calls sk_busy_loop_end. Simultaneous ep_send_events drains rdllist under ep->lock, so busy loop can starve ready list transfers and keep CPU busy while events not delivered, leading to CPU hog DoS across tenants.","classification":"hardening","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:396-400: busy_loop_ep_timeout uses ep fields without locking","fs/eventpoll.c:1905-1990: ep_poll calls ep_busy_loop outside main lock"],"derived_from":[],"proposed_fix_summary":"Protect busy poll preference reads with READ_ONCE and bounding iterations; consider rate limiting busy loop when rdllist not drained."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0011","title":"Max_user_watches bypass via concurrent reference stealing","concept":"per-user epoll_watches increment occurs before attach_epitem; failure paths under race may leak increments allowing quota bypass","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","rlimit NOFILE high"],"description":"Call epoll_ctl ADD on many fds while injecting faults to fail after percpu_counter_inc, e.g., by racing close causing attach_epitem to return -ENOMEM after allocation retry or reverse_path_check failure. ep_insert increments epoll_watches before allocations and only decrements on some error paths. Raced failures can leak increments, letting attacker exceed max_user_watches and retain large numbers of epitems beyond sysctl limit.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1578-1585: user watch counter incremented before allocations and not rolled back on some failures","fs/eventpoll.c:1623-1627: reverse_path_check failure after attach leaves increment unless removal succeeds under races"],"derived_from":[],"proposed_fix_summary":"Defer epoll_watches increment until after successful insertion or guarantee rollback on all exits including reverse_path_check."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0012","title":"Epoll ctl during ep_free leading to ep->mtx use-after-destroy","concept":"ep_free destroys mutex; concurrent epoll_ctl may hold stale pointer when refcount drops unexpectedly","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Force ep_refcount to zero prematurely by triggering ep_clear_and_put concurrently with epoll_ctl operations that still hold pointers into ep obtained through fdget but not taking a reference. When ep_free destroys ep->mtx and frees ep, a concurrent epoll_ctl may still attempt to lock ep->mtx causing use-after-free and memory corruption on freed mutex.","classification":"UAF","impact":"kernel crash","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:815-823: ep_free destroys mutex and frees ep after refcount test","fs/eventpoll.c:899-937: ep_clear_and_put decrements refcount while other threads may still operate"],"derived_from":[],"proposed_fix_summary":"Take strong references (ep_get) in epoll_ctl paths before using ep pointer; guard ep_free with grace period."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0013","title":"EPOLLEXCLUSIVE starvation combined with oneshot resets","concept":"Exclusive wakeups can be rearmed via ONESHOT without clearing waitqueue state leading to perpetual suppression of other waiters","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","EPOLLEXCLUSIVE"],"description":"Register multiple EPOLLEXCLUSIVE waiters on a pipe and rearm using EPOLLONESHOT from userspace threads that immediately re-add after handling. Because ep_poll sets TASK_INTERRUPTIBLE and removes wait entry each wake, ONESHOT user re-arms quickly and reuses rdllink while other waiters remain blocked, effectively monopolizing wakeups and causing starvation DoS for other epoll users monitoring same fd.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1969-2004: exclusive wait queue behaviour documented in comments","fs/eventpoll.c:1666-1676: rdllist insertion for ready events without fairness tracking"],"derived_from":[],"proposed_fix_summary":"Enforce fairness counters or rate limit rearm of EPOLLEXCLUSIVE/ONESHOT combinations."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0014","title":"Poll callback under IRQ uses ep->lock and can deadlock with mtx held in syscalls","concept":"ep_poll_callback takes ep->lock in IRQ context while epoll_ctl holds ep->mtx and may also try to grab lock, allowing deadlock with nested events","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","devices delivering IRQ-based poll callbacks"],"description":"Attacker arranges for a driver poll callback to fire while performing epoll_ctl holding ep->mtx. ep_poll_callback grabs ep->lock with irqsave; if epoll_ctl path later needs to wait on events or busy_poll, lock inversion is possible especially with nested epoll sets causing spinlock recursion. This can hang CPUs servicing interrupts, resulting in DoS.","classification":"deadlock","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1096-1165: ep_poll_callback executes in IRQ and takes ep->lock","fs/eventpoll.c:1564-1699: ep_insert holds ep->mtx while callbacks may run"],"derived_from":[],"proposed_fix_summary":"Limit work done in poll callbacks and avoid holding ep->mtx across operations that can trigger callbacks; use trylock with deferral."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0015","title":"RDLLIST removal without memory barriers causes missed wakeups","concept":"ep_pm_stay_awake and wakeups rely on list_empty checks without smp barriers allowing waiters to miss transitions","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","SMP"],"description":"Stress multiple CPUs delivering ep_poll_callback while ep_send_events moves entries. The ready list empty check uses list_empty_careful without full barrier; combined with ep_poll waitqueue non-barrier wakeups, a thread can miss ready items and go to sleep, causing indefinite wait or repeated busy_loop spinning.","classification":"race-condition","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:390-394: list_empty_careful check without locking","fs/eventpoll.c:1969-2014: waits depend on accurate eavail"],"derived_from":[],"proposed_fix_summary":"Use smp_mb between rdllist updates and availability checks; signal waiters after list updates under lock."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0016","title":"Reverse path check O(N^2) leading to unbounded latency and RCU stall","concept":"reverse_path_check walks tfile_check_list under mutex and can be forced to extreme size causing soft lockups","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","ability to add many fds"],"description":"Attacker adds tens of thousands of FDs with nested epoll topologies just below EP_MAX_NESTS while forcing ep_loop_check full_check. reverse_path_check iterates tfile_check_list under epnested_mutex without cond_resched, leading to long stalls. On RT or PREEMPT_NONE kernels this can trigger RCU stalls and starve other tasks, achieving DoS.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1623-1627 and 2077-2150: reverse_path_check invoked under epnested_mutex without rescheduling"],"derived_from":[],"proposed_fix_summary":"Add cond_resched in reverse_path_check and cap tfile_check_list length per call."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0017","title":"Wakeup_source name leakage across namespaces","concept":"ep_create_wakeup_source uses dentry name snapshot without namespace scoping, leaking host path names to container users","attacker_model":"container_guest","preconditions":["CONFIG_PM_WAKELOCKS","epoll on host-mounted files"],"description":"A container user adds EPOLLWAKEUP on a host path mounted inside the container. ep_create_wakeup_source registers a wakeup source with the dentry name obtained via take_dentry_name_snapshot, which may reveal host-side names not visible in the container's mount namespace. The wakeup source names are observable via sysfs/power, leaking host file layout information.","classification":"info-leak","impact":"info leak","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1500-1507: wakeup source name derived from dentry name regardless of namespace visibility"],"derived_from":[],"proposed_fix_summary":"Sanitize or anonymize wakeup source names for containerized callers; avoid registering names from inaccessible mounts."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0018","title":"EPOLL_CTL_MOD without removing old poll waitqueues causing double callbacks","concept":"ep_modify uses ep_unregister_pollwait and ep_poll_callback re-registration without guarding against concurrent callbacks","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"While epoll_wait runs, attacker issues EPOLL_CTL_MOD toggling edge/level flags. ep_modify unregisters poll wait queues then re-registers via ep_item_poll without ensuring previous callbacks finished. A driver poll callback can run after unregister and before new registration, causing ep_poll_callback to enqueue events while epi->pwqlist is being freed, leading to double insertion or UAF.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1691-1758: ep_modify unregisters and re-registers poll waiters while ep->lock not held","fs/eventpoll.c:1100-1165: ep_poll_callback assumes pwqlist valid"],"derived_from":[],"proposed_fix_summary":"Synchronize ep_modify with poll callbacks using SRCU or per-epi rwsem; defer free until callbacks quiesce."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0019","title":"Epoll fd added to itself via dup+close window","concept":"Race between fget and loop detection when epoll fd duped onto target then original closed lets self-add succeed","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Attacker dup2 epoll fd onto a target fd and quickly closes the original, then calls epoll_ctl ADD using the dup as target. loop_check uses is_file_epoll based on tfile at call time but does not revalidate after fd_install changes. During race, tfile may be NULL or different, letting self insertion slip through until callbacks recurse indefinitely, causing kernel stack blow.","classification":"logic-bypass","impact":"kernel crash","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1564-1684: ep_insert checks is_file_epoll before attach without revalidation after fdget race"],"derived_from":[],"proposed_fix_summary":"Re-fetch and verify target file after acquiring necessary locks or pin file across dup/close operations."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0020","title":"User-triggerable wakeup_source exhaustion","concept":"Each EPOLLWAKEUP registers per-epi wakeup_source without global limit, allowing memory exhaustion via thousands of names","attacker_model":"local_unprivileged","preconditions":["CONFIG_PM_WAKELOCKS","epoll_watches limit high"],"description":"An attacker adds many EPOLLWAKEUP entries pointing to different fds; each ep_create_wakeup_source registers a wakeup source with allocated strings and sysfs entries. There is no per-user or per-epoll limit beyond epoll_watches; thousands of wakeup sources can exhaust kernel memory or sysfs resources, causing system instability.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1629-1635 and 1500-1507: wakeup source allocation per epitem without quota"],"derived_from":[],"proposed_fix_summary":"Enforce per-user wakeup_source quota and free names lazily; reuse shared wakeup sources where possible."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0021","title":"Signal mask restoration confusion with nested epoll_pwait2","concept":"epoll_pwait2 sets user sigmask and restores only when return != -EINTR, enabling signal mask leak across threads when interrupted","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","epoll_pwait2 available"],"description":"Thread A calls epoll_pwait2 with custom sigmask while another thread delivers signals that interrupt the wait. restore_saved_sigmask_unless only restores when return != -EINTR. With nested epoll_pwait2 calls and injected EINTR races, attacker can leave task running with narrowed signal mask unexpectedly, suppressing later signals or altering seccomp interactions, leading to privilege boundary bypass for signal-based isolation.","classification":"logic-bypass","impact":"boundary bypass","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:2477-2495: do_epoll_pwait sets sigmask and only restores when error != -EINTR"],"derived_from":[],"proposed_fix_summary":"Always restore original sigmask in a finally block irrespective of EINTR, or explicitly return masked state to caller."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0022","title":"Compat epoll_pwait endian/type confusion leaking kernel stack","concept":"compat_epoll_pwait copies 32-bit sigset and timespec without struct padding sanitization, potentially leaking stack bytes when copy_to_user of events partially fails","attacker_model":"local_unprivileged","preconditions":["CONFIG_COMPAT","32-bit compat process on 64-bit kernel"],"description":"Craft compat epoll_pwait2 with malformed timeout pointer causing get_timespec64 to fault after ep_poll allocated stack buffers for events. Because compat layer builds epoll_event array on stack and copies out events after partial fill, uninitialized padding from kernel stack can be returned if copy_to_user fails mid-loop, leaking nearby data.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:2509-2520 and 2528-2580: compat epoll_pwait build and copy paths rely on stack structures","fs/eventpoll.c:1118-1170: ep_send_events copies events without clearing padding"],"derived_from":[],"proposed_fix_summary":"Initialize epoll_event buffers before copy_to_user in compat paths or bail out before building events when timeout invalid."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0023","title":"Race between eventpoll_release_file and ep_insert causing dangling pwqlist","concept":"eventpoll_release_file walks file->f_ep without locking ep->mtx; concurrent ep_insert can add pwq to fd after release started","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Force close of a file while another thread is adding it to epoll. eventpoll_release_file sets epi->dying and removes waitqueues but ep_insert can still attach_epitem after check because release does not hold epnested_mutex. The newly attached epi will never get cleaned, leaving pwqlist pointing to freed waitqueue heads and enabling UAF when future callbacks fire.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:446-523: eventpoll_release_file iterates f_ep with minimal locking","fs/eventpoll.c:1527-1604: attach_epitem can succeed mid-release"],"derived_from":[],"proposed_fix_summary":"Synchronize release and insert via refcounted dying flag checked under file->f_lock with global exclusion, rejecting new inserts once release starts."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0024","title":"Epoll ctl ADD bypassing permission checks on anon_inode targets","concept":"anon_inode_getfile returns file without credential-specific checks; ep_insert does not revalidate against caller creds allowing fd sharing to bypass security modules","attacker_model":"container_guest","preconditions":["CONFIG_EPOLL","ability to receive fds via SCM_RIGHTS"],"description":"A privileged helper hands an epoll fd to an unprivileged container. Container duplicates FDs from helper and adds them to its own epoll. Because ep_insert relies on f_op->poll without LSM revalidation and anon_inode files often ignore cred, the container can monitor and potentially wake on helper-owned resources without security module checks, enabling information leaks across namespaces.","classification":"logic-bypass","impact":"boundary bypass","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1564-1684: ep_insert accepts tfile with no credential reevaluation","fs/eventpoll.c:2445-2459: do_epoll_wait trusts file->private_data from caller"],"derived_from":[],"proposed_fix_summary":"Invoke security_file_poll or revalidate credentials when adding foreign fds, especially anon_inodes shared across namespaces."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0025","title":"Ovflist traversal can resurrect detached epitems via stale next pointers","concept":"ovflist uses epi->next single links cleared lazily; callbacks following next after ep_done_scan may re-add freed epitems","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Trigger ep_poll_callback flooding so ovflist overflows. ep_send_events moves ovflist to rdllist and sets epi->next to EP_UNACTIVE_PTR, but callbacks may still run using previous next values without READ_ONCE. An attacker can close and re-add the same fd causing the old epi to be freed via RCU while another CPU traverses ovflist using stale next, leading to UAF and ready list corruption.","classification":"UAF","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1178-1285: ovflist handling sets epi->next without synchronization","fs/eventpoll.c:1660-1678: ready insertion uses rdllink while ovflist may still reference epi"],"derived_from":[],"proposed_fix_summary":"Use READ_ONCE/WRITE_ONCE on epi->next and clear under lock before freeing; guard ovflist traversal with RCU grace period."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0026","title":"Missing overflow check on EP_MAX_EVENTS multiplication","concept":"ep_send_events_user multiplies user-supplied maxevents by epoll_event size without guarding against INT overflow on 32-bit compat","attacker_model":"local_unprivileged","preconditions":["CONFIG_COMPAT","32-bit compat process"],"description":"On 32-bit compat tasks, user may pass maxevents near INT_MAX. EP_MAX_EVENTS is INT_MAX/sizeof(event) but compat layer passes raw maxevents to ep_check_params which only compares >EP_MAX_EVENTS. A negative maxevents due to overflow in userspace or a large value combined with copy_to_user faults can cause ep_send_events_user to compute copy size that wraps, potentially leaking kernel memory by copying more bytes than provided buffer.","classification":"logic-bypass","impact":"info leak","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:241-255: EP_MAX_EVENTS defined as INT_MAX/sizeof(struct epoll_event)","fs/eventpoll.c:2450-2461: ep_check_params used before ep_poll with compat maxevents"],"derived_from":[],"proposed_fix_summary":"Clamp maxevents to EP_MAX_EVENTS for compat after converting to size_t and reject negative values explicitly."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0027","title":"Busy poll budget abuse drains CPU without event delivery","concept":"prefer_busy_poll and busy_poll_budget set per ep allows attacker to keep CPU looping even when sockets drained","attacker_model":"local_unprivileged","preconditions":["CONFIG_NET_RX_BUSY_POLL","sysctl busy_poll_budget"],"description":"Attacker sets busy_poll_budget high via sysctl and attaches many sockets with prefer_busy_poll. ep_busy_loop returns eavail only when ep_events_available; after draining, loop still spins until timeout because budget not decremented when sockets return HZ delay. This CPU spin can be triggered from containers to monopolize cores without generating events, creating DoS.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:396-400: busy_loop_ep_timeout and prefer_busy_poll fields used in loop","fs/eventpoll.c:1978-1980: ep_busy_loop invoked even when no events and relies on busy loop outcome"],"derived_from":[],"proposed_fix_summary":"Decrement budget on each iteration and break when no events produced; enforce per-namespace caps on busy_poll_budget."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0028","title":"Epoll_ctl DEL while ep_scan_ready_list in progress drops refcount twice","concept":"ep_scan_ready_list calls epi_rcu_read_unlock + ep_pm_stay_awake interplay may double drop wakeup_source and refcount when DEL races","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","EPOLLWAKEUP"],"description":"During ep_send_events, ep_scan_ready_list iterates ready items and may drop ep->lock to copy_to_user. If user concurrently issues EPOLL_CTL_DEL, ep_remove_safe frees epi and wakeup_source. ep_scan_ready_list later calls ep_pm_stay_awake/ep_pm_stay_awake_rcu on the same epi pointer, leading to wakeup_source_unregister twice or refcount underflow, causing crashes.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1178-1285: ep_scan_ready_list manipulates ready list and calls ep_pm_stay_awake","fs/eventpoll.c:833-887: __ep_remove frees epi and unregisters wakeup source"],"derived_from":[],"proposed_fix_summary":"Pin epi during scan with RCU reference and check dying flag before wakeup operations; skip entries removed mid-scan."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0029","title":"Threaded epoll_wait with epoll_ctl ADD in same ep causes livelock","concept":"ep_poll busy loop retries when ep_events_available true but ep_try_send_events returns 0; with simultaneous epoll_ctl, readiness check never clears","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"One thread repeatedly epoll_wait with zero timeout while another thread repeatedly adds and removes fds. ep_events_available can return true due to ovflist/rdllist transitions, but ep_try_send_events may return 0 when events consumed mid-loop. The caller spins, consuming CPU and starving other tasks, enabling denial of service from unprivileged user.","classification":"race-condition","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1905-1990: ep_poll loops while eavail true even if try_send returns 0","fs/eventpoll.c:390-394: availability check without lock can be inconsistent"],"derived_from":[],"proposed_fix_summary":"Add retry limit or yield when ep_try_send_events returns 0; recompute under lock."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0030","title":"Overflow of rdllist via uncontrolled ep_poll_callback chaining","concept":"Poll callback adds to rdllist even when already linked; missing guard allows repeated list_add_tail causing list corruption","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","device with pathological poll callback"],"description":"Craft device driver or use TUN sockets to emit rapid poll callbacks while ep_poll is draining rdllist slowly. ep_poll_callback checks ep_is_linked before list_add_tail, but ep_is_linked only inspects rdllink list_empty without locking in some paths. If rdllink corrupted or concurrent list_del_init races, list_add_tail may be called multiple times, corrupting rdllist and triggering BUG_ON in list routines.","classification":"race-condition","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1110-1135: ep_poll_callback adds rdllink if !ep_is_linked under spinlock","fs/eventpoll.c:366-369: ep_is_linked checks list_empty without lock when caller may race"],"derived_from":[],"proposed_fix_summary":"Use list_empty under lock or READ_ONCE flag to guard rdllink linkage; validate before list_add_tail."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0031","title":"Events lost when ep_send_events mixes EPOLLONESHOT rearm and edge-trigger devices","concept":"ep_send_events_user updates epi->event.events for ONESHOT rearm outside device callback visibility, causing missed edges","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","edge-trigger devices"],"description":"Use EPOLLONESHOT with edge-trigger sockets. ep_send_events_user disables ONESHOT and requeues epi only after copying event to user, but device may rearm and signal readiness before ONESHOT flag restored, leading to lost edge and indefinite block. Attacker can exploit to stall services relying on readiness notification, creating denial of service or inconsistent state in user programs.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1186-1258: ep_send_events_user temporarily clears ONESHOT before optional rearm","fs/eventpoll.c:1666-1676: ready insertion assumes event flags stable"],"derived_from":[],"proposed_fix_summary":"Hold ONESHOT flag until user rearm acknowledged or deliver synthetic wakeup when rearm happens after new edge."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0032","title":"Use-after-free via rcu-delayed kfree_rcu of epitem while timerfd callbacks pending","concept":"timerfd poll callback may dereference epi after ep_remove_safe scheduled kfree_rcu; lack of grace period with timer base lock allows UAF","attacker_model":"local_unprivileged","preconditions":["CONFIG_TIMERFD","CONFIG_EPOLL"],"description":"Attach timerfd to epoll and delete it while high-resolution timer callbacks running. timerfd_poll may be invoked after ep_unregister_pollwait but before RCU grace expires. Because ep_poll_callback uses container_of on wait entry referencing epi freed via kfree_rcu, a late timer wake can dereference freed epi leading to crash.","classification":"UAF","impact":"kernel crash","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:833-885: __ep_remove schedules kfree_rcu of epitem","fs/eventpoll.c:1084-1165: ep_poll_callback dereferences epi from wait entry without additional ref"],"derived_from":[],"proposed_fix_summary":"Delay kfree_rcu until all waitqueues drained using call_rcu after removing wait entries with synchronization or fence poll callbacks."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0033","title":"epoll_ctl with EPOLLWAKEUP ignores RLIMIT_MEMLOCK leading to mlock bypass","concept":"wakeup source registration pins memory and sysfs objects but epoll path skips RLIMIT_MEMLOCK accounting","attacker_model":"local_unprivileged","preconditions":["CONFIG_PM_WAKELOCKS"],"description":"An attacker with low RLIMIT_MEMLOCK can still register many EPOLLWAKEUP entries. ep_create_wakeup_source allocates wakeup_source and name without memlock checks, letting attacker pin memory and kernel objects beyond memlock limits, bypassing resource controls.","classification":"resource-exhaustion","impact":"boundary bypass","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:1500-1507 and 1629-1635: wakeup sources allocated per epi with no rlimit enforcement"],"derived_from":[],"proposed_fix_summary":"Charge wakeup_source allocations to memcg and enforce RLIMIT_MEMLOCK or per-user limits."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0034","title":"epoll fd pidfd_getfd race leaks closed ep pointer across namespaces","concept":"do_epoll_wait obtains ep from fd without reference; pidfd_getfd can move fd after fdget leaving stale ep pointer accessible","attacker_model":"container_guest","preconditions":["pidfd_getfd available","CONFIG_EPOLL"],"description":"Thread A in container holds epfd and calls epoll_wait while another thread uses pidfd_getfd from init ns to steal and close the same fd. do_epoll_wait grabs ep pointer before ep_clear_and_put decrements refcount. After theft and close, ep may be freed but ep_poll still uses pointer, leading to UAF and potential data leak across namespaces.","classification":"UAF","impact":"kernel crash","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:2445-2461: do_epoll_wait fetches ep via fd without explicit ep_get","fs/eventpoll.c:815-823 and 899-937: refcount manipulation allows ep_free while others run"],"derived_from":[],"proposed_fix_summary":"Take ep_get when resolving fd in do_epoll_wait and drop after ep_poll; pin file during pidfd_getfd operations."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0035","title":"Memory accounting bypass for epoll user_struct reference","concept":"ep_alloc grabs user_struct but per-user epoll_watches not decremented on some failure paths, letting attacker hoard epoll structs","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Trigger ep_alloc success then force anon_inode_getfile to fail (e.g., by exhausting file descriptors). ep_alloc increments user epoll_watches via ep_insert, but ep_alloc failures only free ep without adjusting per-user structures, enabling attacker to loop creating partially initialized ep structs and exhausting kernel memory unaccounted.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:2166-2195: do_epoll_create allocates ep then anon_inode_getfile may fail, calling ep_clear_and_put but relying on refcount"],"derived_from":[],"proposed_fix_summary":"Ensure ep_alloc failure paths and fd publish errors release user charges and free ep immediately with refcount checks."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0036","title":"Epoll exclusive and wakeup source interplay keeps system awake indefinitely","concept":"EPOLLEXCLUSIVE with EPOLLWAKEUP causes wakeup_source to be held even when events consumed by other waiters","attacker_model":"local_unprivileged","preconditions":["CONFIG_PM_WAKELOCKS","EPOLLEXCLUSIVE"],"description":"Register EPOLLEXCLUSIVE|EPOLLWAKEUP waiters and trigger events that wake only one waiter at a time. ep_pm_stay_awake is called when rdllist is filled, but when exclusive waiters consume events individually, wakeup_source may remain active because ep_pm_stay_awake_rcu lacks matching stay_awake decrement when exclusive path skips list_del. This can keep system out of suspend indefinitely, draining battery.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:1666-1676: ep_pm_stay_awake invoked on rdllist add","fs/eventpoll.c:1186-1258: exclusive handling in ep_send_events_user may not balance wakeup_source"],"derived_from":[],"proposed_fix_summary":"Track wakeup_source activation count per epi and balance on exclusive delivery; clear when events consumed."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0037","title":"epoll_wait timeout underflow allows indefinite blocking bypassing timers","concept":"ep_timeout_to_timespec may convert negative timeout to large ktime leading to unbounded block","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL"],"description":"Pass negative timeout to epoll_wait via raw syscall from compat or ptrace tampering. ep_timeout_to_timespec converts negative to timespec64 with seconds==timeout/1000 without explicit clamp; large negative can wrap to huge positive, causing ep_poll to block almost indefinitely even when caller expected immediate return, enabling DoS by hanging privileged helpers that forward untrusted timeout values.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:2464-2470: ep_timeout_to_timespec used without explicit negative clamp"],"derived_from":[],"proposed_fix_summary":"Validate timeout >=0 before conversion and return -EINVAL for negatives."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0038","title":"Epoll over nfds prevents liveness by starving waiter threads","concept":"max_user_watches enforced via percpu_counter but not tied to scheduler; malicious user can hold thousands of epolls and block housekeeping threads waiting for epnested_mutex","attacker_model":"local_unprivileged","preconditions":["CONFIG_EPOLL","large RLIMIT_NOFILE"],"description":"Create thousands of epolls each near max_user_watches limit and continuously perform nested epoll_ctl linking them. epnested_mutex is global and taken for every loop check; heavy contention can block other threads performing epoll_ctl across system, impacting unrelated tasks and potentially starving system services, leading to DoS without privilege escalation.","classification":"resource-exhaustion","impact":"persistent DoS","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["fs/eventpoll.c:257-264 and 2077-2150: epnested_mutex global used for loop detection affecting all epoll calls"],"derived_from":[],"proposed_fix_summary":"Shard epnested_mutex per user namespace or throttle nested epoll_ctl operations per user to reduce contention."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0039","title":"Cross-ns epoll insertion leaks wakeup_source pointer via sysfs lifetime mismatch","concept":"wakeup_source_unregister in __ep_remove happens after kfree_rcu epi; sysfs entry may persist revealing freed memory contents","attacker_model":"container_guest","preconditions":["CONFIG_PM_WAKELOCKS","access to sysfs power entries"],"description":"Container adds EPOLLWAKEUP to host device, then triggers EPOLL_CTL_DEL while racing with ep_poll_callback. wakeup_source_unregister called after kfree_rcu may remove sysfs entry late; reading sysfs may expose stale stats or even freed memory addresses due to delayed cleanup, leaking host kernel addresses to container.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:833-887: __ep_remove unregisters wakeup source after kfree_rcu","fs/eventpoll.c:1500-1525: wakeup source creation registers sysfs-visible object"],"derived_from":[],"proposed_fix_summary":"Unregister wakeup source before scheduling kfree_rcu and ensure sysfs removal synchronized with container visibility."}
{"file":"fs/eventpoll.c","scenario_id":"fs/eventpoll.c-0040","title":"Busy_poll NAPI ID not cleared on ep_clear_and_put leading to reuse after ep reuse","concept":"When epoll instance destroyed, busy_poll metadata remains in per-ep fields; reusing same memory slab may leak previous napi_id to new ep, enabling unintended busy poll of prior sockets","attacker_model":"local_unprivileged","preconditions":["CONFIG_NET_RX_BUSY_POLL"],"description":"Create epoll with busy poll sockets, then close and free it via ep_clear_and_put. Because ep structure freed without zeroing busy_poll_usecs/napi_id, allocator reuse may return same memory for new ep. ep_busy_loop may then operate with stale napi_id, steering busy polling to unrelated sockets still present, leaking traffic across epoll instances and namespaces.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["fs/eventpoll.c:227-237: busy poll fields stored in eventpoll struct","fs/eventpoll.c:815-823: ep_free frees ep without clearing fields before slab reuse"],"derived_from":[],"proposed_fix_summary":"Zero eventpoll structure in ep_free or allocate from cache with SLAB_POISON; reinitialize busy poll fields on allocation."}
