{ "file": "kernel/bpf/verifier.c", "scenario_id": "kernel/bpf/verifier.c-0001", "title": "Precision Tracking Loss in Subregister Spill/Fill", "concept": "State Management: Precision tracking for scalars is essential for pruning. If a 32-bit subregister spill/fill occurs, the verifier might fail to mark the upper 32-bits as precise or fail to track the 64-bit value's precision dependency, leading to incorrect pruning of a state where a specific value is required for safety but the verifier treats it as a generic scalar.", "attacker_model": "local_unprivileged", "preconditions": ["BPF_JMP32 capability", "Unprivileged BPF enabled or CAP_BPF"], "description": "Vulnerability Path: Setup: Create a scalar register with a specific value (e.g., 0). Spill this register to the stack using a 32-bit store (STW). Fill it back using a 32-bit load (LDW) or 64-bit load (LDX). Trigger: Use this register in a conditional jump that guards an unsafe memory access (e.g., OOB read). Mechanism: The verifier's `mark_chain_precision` logic might not correctly propagate precision requirements through 32-bit stack slots or might lose the link between the 64-bit register value and the 32-bit stack slot. If the verifier marks the register as 'imprecise' in the explored state, it will prune subsequent paths that have different values for this register but share the same 'imprecise' state. Impact: Bypass verifier safety checks, leading to OOB read/write or arbitrary read/write.", "classification": "logic-bypass", "impact": "boundary bypass", "likelihood": "medium", "verdict": "probable_vuln", "context": "kernel_core", "evidence": ["kernel/bpf/verifier.c:19289: if (!regsafe(env, old_reg, cur_reg, idmap, exact))", "kernel/bpf/verifier.c:19082: static bool regsafe(...)"], "derived_from": [], "proposed_fix_summary": "Ensure mark_chain_precision correctly handles all subregister spill/fill widths and propagates precision to the full 64-bit register when necessary." }
{ "file": "kernel/bpf/verifier.c", "scenario_id": "kernel/bpf/verifier.c-0002", "title": "Incorrect 32-bit to 64-bit Bounds Propagation in reg_bounds_sync", "concept": "Integer Handling: The `reg_bounds_sync` function and `__reg_assign_32_into_64` attempt to deduce 64-bit bounds from 32-bit bounds. If this logic is flawed (e.g., incorrect assumptions about sign extension or zero extension during ALU32 ops), a register might have tighter 64-bit bounds in the verifier than in reality.", "attacker_model": "local_unprivileged", "preconditions": ["ALU32 enabled"], "description": "Vulnerability Path: Setup: Perform complex ALU32 operations that result in a wrapped 32-bit value but a specific 64-bit value. Trigger: Use `reg_bounds_sync` (called implicitly after ALU ops) to update 64-bit bounds. Mechanism: If `__reg_assign_32_into_64` incorrectly infers `smin`/`smax` from `s32_min`/`s32_max` without accounting for potential high-bit pollution from previous operations (if zext was optimized out or mishandled), the verifier might believe a register is within [0, 100] when it is actually 0x100000005. Trigger: Use this register as an index for a map access. The verifier approves it (within bounds), but runtime execution accesses OOB. Impact: OOB Read/Write.", "classification": "logic-bypass", "impact": "boundary bypass", "likelihood": "high", "verdict": "probable_vuln", "context": "kernel_core", "evidence": ["kernel/bpf/verifier.c:2760: static void __reg_assign_32_into_64(struct bpf_reg_state *reg)", "kernel/bpf/verifier.c:2688: static void reg_bounds_sync(struct bpf_reg_state *reg)"], "derived_from": [], "proposed_fix_summary": "Review __reg_assign_32_into_64 for all edge cases, especially crossing signed/unsigned boundaries." }
{ "file": "kernel/bpf/verifier.c", "scenario_id": "kernel/bpf/verifier.c-0003", "title": "Pointer Arithmetic Overflow via adjust_ptr_min_max_vals", "concept": "Pointer Arithmetic: `adjust_ptr_min_max_vals` handles `ptr += reg`. It checks for overflows using `check_add_overflow`. However, complex sequences of additions and subtractions, especially involving 32-bit vs 64-bit offsets, might trick the logic into allowing a pointer that wraps around or points before the start of the buffer.", "attacker_model": "local_unprivileged", "preconditions": ["BPF pointer arithmetic allowed"], "description": "Vulnerability Path: Setup: Create a `PTR_TO_MAP_VALUE`. Create a scalar register with a value close to `U64_MAX` or `S64_MIN`. Trigger: Add the scalar to the pointer. Mechanism: The verifier checks `check_add_overflow` on the bounds. If the scalar's bounds are loose (e.g., unknown scalar), the verifier might mark the destination as unknown. But if the scalar has 'fake' tight bounds due to a previous pruning error or logic error (Scenario 2), `adjust_ptr_min_max_vals` might approve the arithmetic. Alternatively, if multiple small additions bypass the `BPF_MAX_VAR_OFF` check individually but sum up to an overflow. Impact: OOB Access relative to map value.", "classification": "logic-bypass", "impact": "boundary bypass", "likelihood": "medium", "verdict": "possible_vuln", "context": "kernel_core", "evidence": ["kernel/bpf/verifier.c:14654: static int adjust_ptr_min_max_vals(...)"], "derived_from": ["kernel/bpf/verifier.c-0002"], "proposed_fix_summary": "Ensure cumulative offsets are checked against BPF_MAX_VAR_OFF global limits rigorously." }
