{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0001","title":"ignore_rlimit_data sysctl bypass enables unbounded brk growth","attacker_model":"local_unprivileged","preconditions":["core_param ignore_rlimit_data=1","CONFIG_COMPAT_BRK enabled"],"description":"An unprivileged process enables the ignore_rlimit_data module parameter (world-writable core_param) via sysfs or inherited boot arg and repeatedly raises its brk(). With data rlimit ignored and CONFIG_COMPAT_BRK providing predictable start_brk, the process can grow its heap unchecked, exhausting memory or colliding with other mappings before mmap_min_addr enforcement is reached.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:77-151: ignore_rlimit_data parameter combined with early RLIMIT_DATA check in brk()"],"proposed_fix_summary":"Restrict ignore_rlimit_data to CAP_SYS_RESOURCE and audit non-default boot parameters; refuse enabling from sysfs without privileges."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0002","title":"brk shrink races with concurrent mmap leading to stale VMA assumptions","attacker_model":"local_unprivileged","preconditions":["multi-threaded process","frequent brk shrink/expand"],"description":"Thread A shrinks brk(), dropping mmap_lock via do_vmi_align_munmap() while another thread maps immediately afterwards. The shrinking path assumes brkvma still valid after unlock and updates mm->brk prior to unmap, risking dereferencing a stale VMA if another mapping was inserted before the unlock window.","classification":"race-condition","impact":"kernel crash","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:160-178: brk shrink updates mm->brk then calls do_vmi_align_munmap() which may drop mmap_lock"],"proposed_fix_summary":"Avoid unlock in shrink path or revalidate VMA after unmap; treat mm->brk update as atomic with VMA removal."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0003","title":"MAP_FIXED_NOREPLACE overlap check bypass via racing unmap","attacker_model":"local_unprivileged","preconditions":["MAP_FIXED_NOREPLACE mapping","concurrent unmap in another thread"],"description":"A thread requests MAP_FIXED_NOREPLACE; do_mmap() only checks find_vma_intersection() before proceeding. If another thread unmaps the region after the check but before mmap_region installs the VMA, the original mapping proceeds and may silently overlap assumptions of the other thread, defeating the no-replace contract and enabling controlled aliasing for exploitation of adjacent VMAs.","classification":"race-condition","impact":"data corruption","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:410-413: MAP_FIXED_NOREPLACE only checked once prior to mmap_region"],"proposed_fix_summary":"Hold mmap_lock across both the existence check and installation or revalidate overlap inside mmap_region for MAP_FIXED_NOREPLACE."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0004","title":"PROT_READ implies PROT_EXEC leaks executable mappings on noexec mounts","attacker_model":"local_unprivileged","preconditions":["personality READ_IMPLIES_EXEC","file on mount toggled from noexec to exec by attacker"],"description":"With READ_IMPLIES_EXEC set, do_mmap() upgrades PROT_READ to include PROT_EXEC unless path_noexec() returns true. An attacker remounts or migrates the backing file onto an exec-capable mount after the path_noexec check, leaving an executable mapping created under the assumption of noexec, enabling unexpected code execution in W^X hardened setups.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["mm/mmap.c:351-359: READ_IMPLIES_EXEC adds PROT_EXEC unless path_noexec prevents it"],"proposed_fix_summary":"Bind executable permission to file inode sb flags at fault time or refuse automatic PROT_EXEC elevation when mount flags can change."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0005","title":"sysctl max_map_count bypass via negative map_count wrapping","attacker_model":"local_unprivileged","preconditions":["map_count near INT_MAX","concurrent unmap/map across threads"],"description":"The map_count limit compares mm->map_count to sysctl_max_map_count before mapping. If map_count overflows due to concurrent unmapped VMAs combined with mmu_gather accounting anomalies, the check can wrap and allow additional mappings beyond the configured limit, enabling resource exhaustion or side channels that assume the cap is enforced.","classification":"refcount","impact":"persistent DoS","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:376-379: map_count compared directly against sysctl_max_map_count without overflow guard"],"proposed_fix_summary":"Use saturating or atomic map_count with overflow detection and treat negative/overflowed values as limit violations."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0006","title":"memlock limit bypass with MAP_LOCKED and concurrent RLIMIT drop","attacker_model":"local_unprivileged","preconditions":["MAP_LOCKED mmap","thread lowers RLIMIT_MEMLOCK after mmap"],"description":"MAP_LOCKED requests check can_do_mlock() and mlock_future_ok() before allocating pages. Another thread can raise RLIMIT_MEMLOCK temporarily to pass the check, perform large MAP_LOCKED mappings, then drop the limit. Already-locked pages remain, enabling locked-page quota exhaustion beyond current limits and starving the system of reclaimable memory.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:415-420: MAP_LOCKED relies on immediate mlock quota check without later enforcement"],"proposed_fix_summary":"Charge locked pages dynamically or revalidate against RLIMIT_MEMLOCK on unlock/reclaimer paths; restrict raising limits without CAP_IPC_LOCK."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0007","title":"memfd seal check race with MAP_FIXED replacing sealed range","attacker_model":"local_unprivileged","preconditions":["sealed memfd with F_SEAL_WRITE","MAP_FIXED mapping overlapping existing VMA"],"description":"memfd_check_seals_mmap() runs before do_vmi_munmap() replaces an existing VMA. A MAP_FIXED mapping against a sealed memfd may pass sealing checks but then unmap an adjacent writable anonymous VMA, merging and reopening write paths despite the seal, allowing modification of sealed contents via aliasing.","classification":"race-condition","impact":"data corruption","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:486-491: seal check occurs prior to potential replacement unmap"],"proposed_fix_summary":"Revalidate seals after do_vmi_munmap or prohibit replacing sealed regions with MAP_FIXED unless exact VMA preserved."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0008","title":"pgoff overflow with huge len permitting wraparound file mapping","attacker_model":"local_unprivileged","preconditions":["file supporting huge len","pgoff near U64_MAX>>PAGE_SHIFT"],"description":"do_mmap() only checks (pgoff + len>>PAGE_SHIFT) < pgoff to detect overflow. For huge files where len is forced to PAGE_ALIGN and not zero, crafted pgoff/len combinations can still wrap when len is later expanded (e.g., huge page alignment), causing file_mmap_ok() to accept mappings that extend beyond the real file and exposing unrelated file data or faults.","classification":"logic-bypass","impact":"info leak","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:367-375: single overflow check on pgoff+len pages","mm/mmap.c:245-275: file_mmap_ok uses maxsize subtraction that can underflow"] ,"proposed_fix_summary":"Use 128-bit arithmetic for pgoff+len, recheck after alignment adjustments, and cap against inode size."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0009","title":"MAP_SHARED with append-only inode still allows executable COW","attacker_model":"local_unprivileged","preconditions":["append-only file with FMODE_READ","READ_IMPLIES_EXEC personality"],"description":"do_mmap() rejects write mappings of append-only files but allows MAP_SHARED with execute permission when READ_IMPLIES_EXEC upgrades PROT bits. An attacker maps an append-only binary with MAP_SHARED|PROT_EXEC and then relies on COW faults or write-like operations through loopholes to execute self-modified code, bypassing append-only semantics.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:455-463: append-only check only tied to FMODE_WRITE","mm/mmap.c:351-359: READ_IMPLIES_EXEC can add PROT_EXEC on shared maps"],"proposed_fix_summary":"Disallow PROT_EXEC MAP_SHARED mappings on append-only files or enforce immutable executable view."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0010","title":"VM_DROPPABLE misuse to bypass fork/cores and leak freed pages","attacker_model":"local_unprivileged","preconditions":["kernel built with VM_DROPPABLE support","attacker uses MAP_DROPPABLE"],"description":"MAP_DROPPABLE disallows combining with locking or stacks but otherwise permits untrusted mappings whose pages may disappear. Attackers can map secrets with MAP_DROPPABLE, trigger reclaim to free them, then rely on missing fork/dump copy semantics to leak timing or stale physical contents when remapped, bypassing expected copy-on-fork isolation.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:503-519: MAP_DROPPABLE forbids some flags yet allows volatile mappings without fork/dump participation"],"proposed_fix_summary":"Document MAP_DROPPABLE semantics and force zeroing of dropped pages; prevent reuse without clearing."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0011","title":"mmap_min_addr bypass via legacy layout and topdown fallback","attacker_model":"local_unprivileged","preconditions":["sysctl legacy_va_layout=1","large stack limits forcing topdown fallback"],"description":"generic_get_unmapped_area_topdown() falls back to bottom-up allocation when topdown fails. With legacy_va_layout enabled, info.low_limit becomes TASK_UNMAPPED_BASE and may allow allocations near zero if mmap_min_addr is small and stack_guard_gap minimal, enabling NULL-deref exploitation paths.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:736-786: topdown allocator fallback resets low_limit to TASK_UNMAPPED_BASE with mmap_min_addr check only on length"],"proposed_fix_summary":"Enforce mmap_min_addr on fallback results regardless of legacy layout; refuse allocations below a safe threshold."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0012","title":"stack_guard_gap misplacement allows stack/heap collision","attacker_model":"local_unprivileged","preconditions":["small stack_guard_gap","custom MAP_FIXED near stack"],"description":"The stack_guard_placement() result is used as start_gap for unmapped area search. Attackers can craft MAP_FIXED mappings immediately below the current stack when stack_guard_gap is small, then trigger stack growth to collide with the mapping, inducing controlled corruption in adjacent VMAs.","classification":"race-condition","impact":"privilege escalation","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:703-709 and 714-717: get_unmapped_area uses stack_guard_placement to compute vm_start_gap before placing mappings"],"proposed_fix_summary":"Enforce minimum guard gaps regardless of sysctl and reject MAP_FIXED placements inside guard distance."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0013","title":"userfaultfd unmap tracking skipped on brk shrink failure","attacker_model":"local_unprivileged","preconditions":["userfaultfd registered on heap","brk shrink fails due to intersecting VMA"],"description":"brk() calls userfaultfd_unmap_complete() only on successful shrink path. If shrink fails after mm->brk change, userfaultfd notifications may be missed while memory accounting already adjusted, causing userfaultfd clients to access stale ranges and enabling UAF-like desync between fault handler and kernel state.","classification":"logic-bypass","impact":"kernel crash","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:160-178 and 201-207: userfaultfd_unmap_complete invoked only on success_unlocked"],"proposed_fix_summary":"Ensure userfaultfd notifications or rollbacks occur on all exit paths where mm->brk is mutated."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0014","title":"MAP_SYNC accepted on non-block devices enabling fs-dax expectations","attacker_model":"local_unprivileged","preconditions":["MAP_SHARED mapping on regular file","filesystem lacks MAP_SYNC support"],"description":"MAP_SYNC is filtered only when MAP_SHARED without MAP_SHARED_VALIDATE by masking flags. For files where f_op lacks FOP_MMAP_SYNC, unsupported MAP_SYNC is silently ignored, misleading applications into assuming durability guarantees and potentially corrupting data when used with direct writes or pmem-like media emulated via drivers.","classification":"logic-bypass","impact":"data corruption","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:430-448: MAP_SYNC only validated under MAP_SHARED_VALIDATE or FOP_MMAP_SYNC"],"proposed_fix_summary":"Reject MAP_SYNC unless MAP_SHARED_VALIDATE used and file advertises support; emit audit warnings."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0015","title":"path_noexec TOCTOU allows executable mapping after remount","attacker_model":"local_unprivileged","preconditions":["attacker can remount filesystem","existing mmap with VM_MAYEXEC cleared"],"description":"do_mmap() clears VM_MAYEXEC when path_noexec() is true at map time. If the filesystem is later remounted exec, the VMA retains VM_MAYEXEC cleared but may still allow execute faults because PROT_EXEC was not stripped when remount occurs, enabling execution from formerly noexec media without further checks.","classification":"TOCTOU","impact":"privilege escalation","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["mm/mmap.c:469-474: VM_MAYEXEC cleared based on path_noexec result at mapping time"],"proposed_fix_summary":"Tie executable permission to superblock flags at fault time or re-evaluate on remount; consider denying exec on remounted paths until remap."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0016","title":"pkey execute-only bypass when execute_only_pkey fails","attacker_model":"local_unprivileged","preconditions":["PKU supported","execute_only_pkey returns negative"],"description":"When prot==PROT_EXEC, do_mmap() calls execute_only_pkey(); negative returns reset pkey to 0 but mapping proceeds with PROT_EXEC. On architectures where execute-only requires special permissions, failing to acquire a pkey silently downgrades to normal executable mapping, bypassing execute-only intent and weakening W^X.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:390-394: negative execute_only_pkey results ignored and pkey set to 0"],"proposed_fix_summary":"Propagate execute_only_pkey failure to userspace or clear PROT_EXEC when execute-only cannot be honored."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0017","title":"VM_GROWSDOWN mapping with MAP_SHARED disallowed but error paths leak accounting","attacker_model":"local_unprivileged","preconditions":["MAP_SHARED|MAP_GROWSDOWN attempt","rlimit bypass via partial failure"],"description":"The code rejects MAP_SHARED combined with VM_GROWSDOWN by returning -EINVAL, but page accounting may have been adjusted by earlier steps (len alignment, map_count check). An attacker can spam failing calls to perturb vm_committed_as and trigger OOM heuristics or leak side-channel timing on accounting saturation.","classification":"DoS","impact":"kernel crash","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:493-502 and 375-379: MAP_SHARED with VM_GROWSDOWN rejected after map_count/length adjustments"],"proposed_fix_summary":"Defer accounting changes until after flag validation or unwind on failure; add rate limiting on invalid mmap sequences."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0018","title":"can_mmap_file hook bypass for F_OP lacking mmap sanity","attacker_model":"local_unprivileged","preconditions":["custom char device with mmap","driver forgets to implement can_mmap_file"],"description":"do_mmap() calls can_mmap_file(file) but many drivers default to allowing mapping. Attackers with access to misc devices can map arbitrary physical or kernel memory exposed by buggy f_ops without permission checks, reintroducing /dev/mem-like access via drivers that rely on VM flags alone.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:475-479: can_mmap_file() gate before VM flag validation"],"proposed_fix_summary":"Require f_op->mmap implementations to opt-in via capabilities or LSM, and default-deny can_mmap_file for unknown drivers."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0019","title":"anonymous MAP_SHARED zero-length handling returns -ENOMEM but leaks hint alignment timing","attacker_model":"local_unprivileged","preconditions":["MAP_SHARED anonymous mapping with len near overflow"],"description":"do_mmap() returns -ENOMEM when len aligns to zero after PAGE_ALIGN. Attackers can vary lengths around ULONG_MAX to measure timing differences in overflow detection and leak kernel PAGE_SIZE and rounding behavior for ASLR-related side channels.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"not_feasible","context":"kernel_core","evidence":["mm/mmap.c:347-370: len PAGE_ALIGN and zero-length checks"],"proposed_fix_summary":"Normalize error handling and constant-time responses for overflow/zero cases to reduce side-channel observability."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0020","title":"pgoff truncation for MAP_SHARED sockets allows overlay of unrelated mapping","attacker_model":"local_unprivileged","preconditions":["mapping socket via mmap","non-zero pgoff"],"description":"file_mmap_ok() treats sockets as MAX_LFS_FILESIZE and ignores pgoff semantics. An attacker can pass crafted pgoff to map unrelated kernel socket buffers or driver memory if socket mmap handler fails to validate pgoff, enabling info leaks from kernel-controlled pages shared via VM_SHARED.","classification":"logic-bypass","impact":"info leak","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:247-255 and 264-275: sockets treated like regular files with max size permitting large pgoff"],"proposed_fix_summary":"Enforce pgoff validation inside socket mmap handlers and cap offsets for non-file inodes."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0021","title":"unmap-vs-exit race leaves detached VMAs referenced","attacker_model":"local_unprivileged","preconditions":["thread exits while another thread mremaps same mm"],"description":"exit_mmap() walks VMAs under read then write locks, marking detached before remove_vma. A concurrent mremap using vma references after detachment could operate on freed structures because detach marks occur before mmu_notifier release is complete, leading to UAF.","classification":"race-condition","impact":"kernel crash","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["mm/mmap.c:1259-1305: exit_mmap detaches and removes VMA while other users may still hold references"],"proposed_fix_summary":"Ensure VMA references are RCU-freed only after synchronizing with concurrent operations; add refcounting or blocking of mremap during exit."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0022","title":"mmu_notifier_release missing failure path recovery in exit_mmap","attacker_model":"local_unprivileged","preconditions":["registered mmu_notifier driver that can fail","process exit"],"description":"exit_mmap() calls mmu_notifier_release() without checking for errors; notifier failures could leave devices still referencing the mm after VMAs have been unmapped, allowing DMA into freed page tables or stale address spaces.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:1259-1270: mmu_notifier_release() called without error handling before unmapping"],"proposed_fix_summary":"Handle notifier failures by aborting teardown or forcing device quiesce; propagate errors to prevent freeing active mm."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0023","title":"vm_unmapped_area alignment for huge pages leaks physical alignment info","attacker_model":"local_unprivileged","preconditions":["mapping hugetlb file","align_mask reflects huge page size"],"description":"When mapping hugepages, generic_get_unmapped_area sets align_mask to huge_page_mask_align(). The allocator's search timing and success reveal hugepage alignment and availability, which can be exploited for ASLR reduction and cross-VM side channels on shared hosts.","classification":"info-leak","impact":"info leak","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:715-718: align_mask derived from huge page properties in vm_unmapped_area search"],"proposed_fix_summary":"Randomize alignment padding for hugetlb mappings or mask timing via constant-time search when alignment requested."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0024","title":"MAP_FIXED bypass of sealing during do_vmi_munmap error paths","attacker_model":"local_unprivileged","preconditions":["MAP_FIXED on sealed memfd","do_vmi_munmap fails mid-way"],"description":"Sealing is verified before unmapping existing ranges. If do_vmi_munmap fails (e.g., due to userfaultfd or concurrent lock), the function returns error but may have partially unmapped a region, temporarily violating seals and exposing writable windows before rollback completes.","classification":"race-condition","impact":"data corruption","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:486-491 with MAP_FIXED comments at 381-388 indicating unmap in mmap_region"],"proposed_fix_summary":"Perform sealing checks again after unmap success and ensure atomic replacement or full rollback on failure."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0025","title":"legacy_va_layout exposes predictable mmap base undermining ASLR","attacker_model":"local_unprivileged","preconditions":["sysctl legacy_va_layout=1","32-bit compat process"],"description":"With legacy layout, generic_get_unmapped_area_topdown uses mm->mmap_base set predictably near TASK_UNMAPPED_BASE, reducing address space randomization bits. Attackers can brute-force JIT-ROP mappings or spray predictable addresses to bypass ASLR.","classification":"info-leak","impact":"privilege escalation","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:736-786 and sysctl entries 1506-1548 controlling legacy_va_layout"] ,"proposed_fix_summary":"Deprecate legacy_va_layout or require CAP_SYS_ADMIN to set; increase randomization bits even in legacy mode."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0026","title":"max_map_count sysctl writable by unprivileged namespaces causes DoS","attacker_model":"container_guest","preconditions":["user namespace with sysctl vm writable","kernel configured to allow"],"description":"sysctl table exposes max_map_count writable at mode 0644. In user namespaces where sysctl write is allowed, an unprivileged container can raise max_map_count dramatically, enabling massive VMA spray and host-wide memory exhaustion despite namespace isolation.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"medium","verdict":"probable_vuln","context":"kernel_core","evidence":["mm/mmap.c:1506-1514: sysctl max_map_count writable 0644"],"proposed_fix_summary":"Restrict vm sysctls to init_user_ns or CAP_SYS_ADMIN in init; ignore namespace writes for global limits."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0027","title":"mmap_rnd_bits sysctl writeable enabling ASLR reduction","attacker_model":"local_unprivileged","preconditions":["access to /proc/sys/vm/mmap_rnd_bits","CONFIG_HAVE_ARCH_MMAP_RND_BITS"],"description":"mmap_rnd_bits sysctl is writable with mode 0600 but accessible to the current user. A compromised but unprivileged account can lower randomization bits for its own tasks and children, simplifying exploitation of other bugs and potentially shared services running under same uid.","classification":"hardening","impact":"privilege escalation","likelihood":"high","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:1528-1534: mmap_rnd_bits sysctl writable 0600"],"proposed_fix_summary":"Disallow lowering randomization below secure defaults or require CAP_SYS_ADMIN to decrease bits."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0028","title":"userfaultfd interaction with MAP_DROPPABLE unmapped pages","attacker_model":"local_unprivileged","preconditions":["userfaultfd registered","MAP_DROPPABLE mapping"],"description":"MAP_DROPPABLE mappings omit copying on fork and can vanish. userfaultfd handlers may still expect faults for missing pages; sudden disappearance without notifier integration can deadlock fault handlers or allow malicious userfaultfd server to read freed memory before zeroing.","classification":"race-condition","impact":"info leak","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:503-520: droppable mappings skip fork/dump semantics; userfaultfd tracking not mentioned"],"proposed_fix_summary":"Integrate droppable VMAs with userfaultfd notifications and zero freed pages before reuse."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0029","title":"VM_ACCOUNT mismatch on exit_mmap undercounts committed memory","attacker_model":"local_unprivileged","preconditions":["VM_ACCOUNT set on VMAs","process exits during memory pressure"],"description":"exit_mmap() increments nr_accounted when VM_ACCOUNT set but relies on vma_pages(). If VM_ACCOUNT was cleared incorrectly earlier, committed accounting is skipped and vm_unacct_memory later frees too little, leading to accounting inconsistency that attackers can exploit to bypass overcommit limits by flipping flags via mprotect operations.","classification":"logic-bypass","impact":"persistent DoS","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:1298-1315: VM_ACCOUNT accumulation and vm_unacct_memory call"],"proposed_fix_summary":"Track committed charges separately from VM flags and validate accounting during mprotect or VMA merges."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0030","title":"__get_unmapped_area ignores arch_mmap_check side effects","attacker_model":"local_unprivileged","preconditions":["architecture with custom arch_mmap_check that rejects late"],"description":"round_hint_to_min and __get_unmapped_area run before arch_mmap_check validations in arch code. If arch_mmap_check assumes invariants that later steps violate (e.g., address randomization range), the check may be bypassed when MAP_FIXED rewrites placement after initial approval, permitting mappings into reserved ranges like vDSO or guard holes.","classification":"logic-bypass","impact":"privilege escalation","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:360-407: address selection performed prior to deeper arch validation hooks"],"proposed_fix_summary":"Invoke arch_mmap_check after final address determination including MAP_FIXED handling."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0031","title":"MAP_SHARED_VALIDATE limited flags silently masked enabling undefined persistence","attacker_model":"local_unprivileged","preconditions":["filesystem returns FOP_MMAP_SYNC","application assumes MAP_SYNC honored"],"description":"When MAP_SHARED is used with non-legacy flags but without MAP_SHARED_VALIDATE, unsupported flags are silently ignored. Applications believing MAP_SYNC or other bits are enforced may store sensitive data assuming durability, but kernel may revert to legacy semantics causing silent loss or corruption exploitable by attacker controlling crash timing.","classification":"logic-bypass","impact":"data corruption","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:434-448: MAP_SHARED path masks flags instead of rejecting"],"proposed_fix_summary":"Reject unsupported flags unless MAP_SHARED_VALIDATE explicitly set; log warnings for masked flags."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0032","title":"MAP_PRIVATE executable mapping on noexec fs via re-mprotect after remount","attacker_model":"local_unprivileged","preconditions":["filesystem toggled from noexec to exec","existing VM_MAYEXEC cleared"],"description":"Private mappings from noexec files have VM_MAYEXEC cleared. After remount exec, attacker calls mprotect(PROT_EXEC); since vm_flags no longer carry may-exec, mprotect may deny, but if vm_flags were merged with an adjacent executable VMA, VM_MAYEXEC might be restored, enabling execution of previously non-exec file content.","classification":"race-condition","impact":"privilege escalation","likelihood":"low","verdict":"probable_vuln","context":"kernel_core","evidence":["mm/mmap.c:469-474: VM_MAYEXEC cleared at map time; adjacent VMA merges may reintroduce flags in merge logic (calc_vm_flag_bits)"],"proposed_fix_summary":"Disallow merging VMAs with differing may-exec provenance or re-evaluate mount flags during merges and mprotect."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0033","title":"droppable mappings bypass core dump exclusion and leak secrets","attacker_model":"local_unprivileged","preconditions":["MAP_DROPPABLE used to store secrets","process crashes"],"description":"VM_DROPPABLE VMAs skip dump and fork. If a process stores secrets there and then crashes, core dump filters may still include neighboring VMAs exposing metadata or partial contents; meanwhile, droppable pages might not be cleared, letting forensic tools recover data thought to be absent.","classification":"info-leak","impact":"info leak","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:503-520: droppable VMAs avoid dump/copy semantics"],"proposed_fix_summary":"Explicitly mark droppable VMAs as MADV_DONTDUMP and zero on release; document security caveats."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0034","title":"get_unmapped_area zero-page optimisation reveals mmap base via timing","attacker_model":"local_unprivileged","preconditions":["frequent mmap with addr=0","measure timing"],"description":"generic_get_unmapped_area returns provided hint quickly if gap exists, otherwise performs search. Timing differences leak layout of existing VMAs and mmap_base, reducing ASLR for exploits needing approximate addresses.","classification":"info-leak","impact":"privilege escalation","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:703-709 and 712-719: early return when hint fits versus full vm_unmapped_area search"],"proposed_fix_summary":"Add jitter or constant-time behaviour for mmap hint handling or randomize mmap_base more frequently."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0035","title":"MAP_DROPPABLE with hugetlb not rejected early enough","attacker_model":"local_unprivileged","preconditions":["MAP_DROPPABLE|MAP_HUGETLB attempt"],"description":"The code rejects droppable combined with MAP_HUGETLB but only after multiple calculations and get_unmapped_area calls may occur in some paths. Attackers can stress allocator paths with invalid combinations to consume CPU and fragment address space, leading to DoS.","classification":"DoS","impact":"persistent DoS","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:503-520: MAP_DROPPABLE rejects hugetlb but after some preliminary checks and alignments"],"proposed_fix_summary":"Fail fast for unsupported flag combinations before heavy address searches."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0036","title":"pgoff alignment for huge page mask misuses sign-extending shift","attacker_model":"local_unprivileged","preconditions":["mapping with large pgoff causing sign issues on 32-bit"],"description":"When hugepage alignment is requested, align_mask is derived from huge_page_mask_align() and applied to pgoff-derived addresses. On 32-bit, large pgoff may sign-extend or wrap, leading to address selection outside user range yet passing initial checks, causing fault in kernel or mapping of reserved regions.","classification":"logic-bypass","impact":"kernel crash","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:715-718 and 766-772: align_mask applied to addr search without explicit overflow handling"] ,"proposed_fix_summary":"Use 64-bit intermediate arithmetic and validate resulting address against user limits before mapping."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0037","title":"sysctl_user_reserve_kbytes calculation assumes accurate free pages","attacker_model":"local_unprivileged","preconditions":["control over NUMA node pressure","OVERCOMMIT_NEVER"],"description":"init_user_reserve computes sysctl_user_reserve_kbytes from global free pages at boot. Attackers that exhaust a subset of nodes before boot (e.g., via kexec with pinned pages) can skew free count low, leading to small reserve and easier OOM denial after boot by consuming vm space via mmap/munmap churn.","classification":"hardening","impact":"persistent DoS","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:1576-1584: reserve calculated once during init from free pages"],"proposed_fix_summary":"Recalculate reserves periodically or base on total managed pages rather than snapshot of free pages."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0038","title":"sysctl_admin_reserve_kbytes insufficient on small systems under namespaces","attacker_model":"container_guest","preconditions":["small-memory host","user namespace mapping"],"description":"admin_reserve is set based on free pages during init. In container scenarios with strict limits, the global reserve may be unavailable, letting a malicious container exhaust memory via many VMAs while sysctl_admin_reserve_kbytes remains too small to permit recovery, causing host-level DoS.","classification":"hardening","impact":"persistent DoS","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:1587-1595: admin reserve uses global free page snapshot and not namespace-aware"],"proposed_fix_summary":"Make reserves cgroup-aware or enforce per-memcg reserve to keep host admin responsive."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0039","title":"execute-only pkey fallback leaks instruction side channels","attacker_model":"local_unprivileged","preconditions":["PKU hardware present","mapping PROT_EXEC only"],"description":"If execute_only_pkey returns 0 for supported hardware, mappings get default pkey and may still be readable via speculative or side-channel techniques because no extra protection is applied. Lack of enforcement can expose JIT code to reads despite intended execute-only policy.","classification":"info-leak","impact":"info leak","likelihood":"medium","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:390-394: execute-only path defaults pkey to 0 allowing read permission if hardware permits"],"proposed_fix_summary":"Force VM_ACCESS_FLAGS to remove read on execute-only requests or fail mapping when execute-only cannot be enforced."}
{"file":"mm/mmap.c","scenario_id":"mm/mmap.c-0040","title":"vma_set_page_prot racing with remove_protection_ptes","attacker_model":"local_unprivileged","preconditions":["concurrent mprotect and mremap operations"],"description":"vma_set_page_prot updates vm_page_prot with WRITE_ONCE to satisfy remove_protection_ptes reading without mmap_lock. If a concurrent thread modifies vm_flags and triggers vma_set_page_prot while remove_protection_ptes reads, inconsistent protections could be applied, leaving writable mappings where write-notify should be set, enabling COW bypass.","classification":"race-condition","impact":"data corruption","likelihood":"low","verdict":"hardening_only","context":"kernel_core","evidence":["mm/mmap.c:80-93: vma_set_page_prot uses WRITE_ONCE to update vm_page_prot for lockless readers"],"proposed_fix_summary":"Add explicit barriers or take lock around remove_protection_ptes when vm_page_prot can change; avoid lockless read during transitions."}
