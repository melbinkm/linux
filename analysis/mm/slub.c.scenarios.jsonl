{ "file": "mm/slub.c", "scenario_id": "mm/slub.c-0001", "title": "Unbounded Memory Pinning via Barn Overflow Race in Per-CPU Sheaves", "concept": "Concurrency/Lifecycle: The node_barn limit check for full sheaves is racy, allowing concurrent RCU callbacks to overflow the limit and pin unbounded amounts of memory.", "attacker_model": "local_unprivileged", "preconditions": ["CONFIG_SLUB_CPU_SHEAVES (enabled by default on some configs or via slab_debug options)", "High load triggering RCU frees"], "description": "Vulnerability Path: Setup: A kmem_cache is created with cpu_sheaves enabled (e.g., via specific flags or default configuration). Trigger: An attacker triggers massive parallel freeing of objects (e.g., closing file descriptors) that use call_rcu (SLAB_TYPESAFE_BY_RCU or similar flows). Mechanism: In `rcu_free_sheaf`, the code checks `if (data_race(barn->nr_full) < MAX_FULL_SHEAVES)`. This check is lockless. If it passes, it calls `barn_put_full_sheaf` which unconditionally acquires the lock and adds the sheaf to the list, incrementing `nr_full`. Multiple concurrent RCU callbacks can all see `nr_full < 10` and proceed to add their sheaves, causing `nr_full` to grow far beyond the limit. Since objects in sheaves are considered 'in-use' by the slab allocator, the underlying slab pages cannot be reclaimed by the system, leading to memory exhaustion (OOM).", "classification": "race-condition", "impact": "persistent DoS", "likelihood": "medium", "verdict": "confirmed_vuln", "context": "driver", "evidence": ["mm/slub.c:2666: if (data_race(barn->nr_full) < MAX_FULL_SHEAVES)", "mm/slub.c:2668: barn_put_full_sheaf(barn, sheaf)"], "derived_from": [], "proposed_fix_summary": "Re-check `barn->nr_full` inside the spinlock in `barn_put_full_sheaf` or `rcu_free_sheaf`, or use atomic increments/decrements with cmpxchg loop for the limit check." }
{ "file": "mm/slub.c", "scenario_id": "mm/slub.c-0002", "title": "Integer Overflow in `calculate_sizes` leading to Divide-by-Zero Crash", "concept": "Input Validation: Integer overflow in ALIGN macro during cache size calculation leads to zero-sized objects and subsequent division by zero.", "attacker_model": "privileged", "preconditions": ["Ability to create kmem_cache with arbitrary size (e.g. via specific drivers or test modules)"], "description": "Vulnerability Path: Setup: Caller invokes `kmem_cache_create` (or `do_kmem_cache_create`) with a large `size` parameter (e.g., UINT_MAX). Trigger: `calculate_sizes` is called. Mechanism: `size = ALIGN(size, sizeof(void *))` is executed. `ALIGN(x, a)` is `(x + a - 1) & ~(a - 1)`. If `size` is UINT_MAX and alignment is 8, `size + 7` overflows to 6, and masking results in 0. The function continues with `s->size = 0`. Later, `calculate_order` calls `order_objects`, which calculates `((unsigned int)PAGE_SIZE << order) / size`. Division by zero occurs, crashing the kernel.", "classification": "logic-bypass", "impact": "kernel crash", "likelihood": "low", "verdict": "confirmed_vuln", "context": "kernel_core", "evidence": ["mm/slub.c:7966: size = ALIGN(size, sizeof(void *))", "mm/slub.c:646: return ((unsigned int)PAGE_SIZE << order) / size"], "derived_from": [], "proposed_fix_summary": "Check for overflow before or after ALIGN in `calculate_sizes`, or ensure `size` is non-zero after alignment." }
{ "file": "mm/slub.c", "scenario_id": "mm/slub.c-0003", "title": "Type Confusion via Cross-Cache Freeing with Hardening Disabled", "concept": "State Management: `kmem_cache_free` allows freeing objects to the wrong cache if hardening is disabled, corrupting freelists.", "attacker_model": "local_unprivileged", "preconditions": ["CONFIG_SLAB_FREELIST_HARDENED is disabled", "SLAB_CONSISTENCY_CHECKS is disabled"], "description": "Vulnerability Path: Setup: A driver or subsystem mistakenly calls `kmem_cache_free(cache_A, obj_from_cache_B)`. Trigger: The free operation proceeds. Mechanism: `cache_from_obj` checks `cachep != s` only if hardening or debug is enabled. If disabled, it returns `s` (cache_A). `slab_free` calls `do_slab_free`. If `virt_to_slab(obj)` belongs to cache_B, `do_slab_free` might notice the mismatch in `c->slab` and fall back to `__slab_free`. `__slab_free` takes the lock of `n` (derived from cache_A) and adds the slab (from cache_B) to cache_A's partial list. Future allocations from cache_A will pop objects from this slab, but they are actually layout B. If size_A != size_B, this leads to heap overflow or info leak.", "classification": "type-confusion", "impact": "boundary bypass", "likelihood": "low", "verdict": "hardening_only", "context": "driver", "evidence": ["mm/slub.c:4516: if (!IS_ENABLED(CONFIG_SLAB_FREELIST_HARDENED) ... return s;"], "derived_from": [], "proposed_fix_summary": "Enforce `cache_from_obj` checks even when hardening is disabled, or at least in `__slab_free` slowpath." }
